<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_11">
  <title>Chapter 11: Performance Optimization</title>
  <body>
    <section><title>Table of Contents</title></section>
    <ul>
      <li>
        11.1 Performance Challenges
      </li>
      <li>
        11.2 Intelligent Caching Strategies
      </li>
      <li>
        11.3 Parallel Execution Optimization
      </li>
      <li>
        11.4 Lazy Loading Patterns
      </li>
      <li>
        11.5 Memory Management
      </li>
      <li>
        11.6 Connection Pooling
      </li>
      <li>
        11.7 Response Streaming
      </li>
      <li>
        11.8 Profiling and Benchmarking
      </li>
      <li>
        Exercises
      </li>
      <li>
        Summary
      </li>
    </ul>
    <section><title>11.1 Performance Challenges</title></section>
    <p>AI coding assistants face unique performance challenges: slow LLM API calls, large context windows, and expensive tool execution.</p>
    <section><title>Performance Bottlenecks</title></section>
    <codeblock outputclass="language-typescript">/**
 * Common performance bottlenecks in AI systems
 */
export enum PerformanceBottleneck {
  /** LLM API calls (1-5 seconds each) */
  API_LATENCY = &apos;api_latency&apos;,

  /** Large prompts and responses */
  LARGE_PAYLOADS = &apos;large_payloads&apos;,

  /** Sequential tool execution */
  SEQUENTIAL_EXECUTION = &apos;sequential_execution&apos;,

  /** Repeated identical operations */
  NO_CACHING = &apos;no_caching&apos;,

  /** Memory leaks from large conversations */
  MEMORY_LEAKS = &apos;memory_leaks&apos;,

  /** Blocking I/O operations */
  BLOCKING_IO = &apos;blocking_io&apos;,

  /** Cold start for lazy-loaded modules */
  COLD_START = &apos;cold_start&apos;
}

/**
 * Performance targets for production
 */
export const PERFORMANCE_TARGETS = {
  // Response time targets
  firstTokenLatency: 500,      // ms - Time to first token
  simpleQuery: 2000,           // ms - Total time for simple queries
  complexQuery: 5000,          // ms - Total time for complex queries

  // Throughput targets
  requestsPerSecond: 10,       // Concurrent requests handled

  // Resource targets
  memoryUsage: 512 * 1024 * 1024, // 512 MB max
  cpuUsage: 80,                // % max

  // Cache targets
  cacheHitRate: 70,            // % - Cache effectiveness

  // Cost targets
  costPerQuery: 0.01           // $ - Average cost per query
};
</codeblock>
    <section><title>Before vs After Optimization</title></section>
    <codeblock outputclass="language-typescript">/**
 * Performance comparison: Before optimization
 */
async function analyzeCodebase_SLOW() {
  // Sequential execution - SLOW!
  const files = await listFiles(&apos;src/&apos;);        // 150ms

  for (const file of files) {                   // 10 files
    const content = await readFile(file);       // 50ms each = 500ms
    const analysis = await analyzeWithAI(content); // 1,500ms each = 15,000ms
    await saveAnalysis(file, analysis);         // 30ms each = 300ms
  }

  // Total: 15,950ms (~16 seconds) ❌
}

/**
 * Performance comparison: After optimization
 */
async function analyzeCodebase_FAST() {
  // Parallel execution with caching
  const files = await listFiles(&apos;src/&apos;);        // 150ms

  // Read all files in parallel
  const contents = await Promise.all(           // 50ms (parallelized)
    files.map(file =&gt; readFile(file))
  );

  // Analyze in parallel with caching
  const analyses = await Promise.all(           // 1,500ms (parallelized + cache hits)
    contents.map((content, i) =&gt; {
      const cacheKey = hashContent(content);
      return getCachedOrAnalyze(cacheKey, content);
    })
  );

  // Save in parallel
  await Promise.all(                            // 30ms (parallelized)
    analyses.map((analysis, i) =&gt;
      saveAnalysis(files[i], analysis)
    )
  );

  // Total: 1,730ms (~1.7 seconds) ✓
  // 9.2x faster! 🚀
}
</codeblock>
    <section><title>Optimization Strategy</title></section>
    <codeblock>┌─────────────────────────────────────────────────────────────┐
│              Performance Optimization Strategy               │
└─────────────────────────────────────────────────────────────┘

1. Measure First
   └─ Profile actual performance
   └─ Identify real bottlenecks
   └─ Set measurable targets

2. Low-Hanging Fruit
   └─ Add caching (biggest impact)
   └─ Parallelize independent operations
   └─ Reduce payload sizes

3. Advanced Optimizations
   └─ Lazy loading
   └─ Connection pooling
   └─ Memory optimization
   └─ Streaming

4. Validate
   └─ Measure improvements
   └─ Monitor in production
   └─ Iterate
</codeblock>
    <section><title>11.2 Intelligent Caching Strategies</title></section>
    <p>Caching is the highest-impact optimization. Cache AI responses, tool results, and expensive computations.</p>
    <section><title>Multi-Level Cache</title></section>
    <codeblock outputclass="language-typescript">/**
 * Multi-level cache with different strategies per level
 */
export class MultiLevelCache {
  private l1Cache: Map&lt;string, CacheEntry&gt;; // Memory cache (fast)
  private l2Cache: LRUCache&lt;string, CacheEntry&gt;; // LRU cache (medium)
  private l3Cache: DiskCache; // Disk cache (slow but persistent)

  constructor(
    private options: CacheOptions
  ) {
    this.l1Cache = new Map();
    this.l2Cache = new LRUCache({
      max: options.l2MaxSize || 1000,
      ttl: options.l2TTL || 1000 * 60 * 60 // 1 hour
    });
    this.l3Cache = new DiskCache(options.l3Path || &apos;.cache&apos;);
  }

  /**
   * Get value from cache (checks all levels)
   */
  async get&lt;T&gt;(key: string): Promise&lt;T | null&gt; {
    // Check L1 (memory)
    const l1Entry = this.l1Cache.get(key);
    if (l1Entry &amp;&amp; !this.isExpired(l1Entry)) {
      return l1Entry.value as T;
    }

    // Check L2 (LRU)
    const l2Entry = this.l2Cache.get(key);
    if (l2Entry &amp;&amp; !this.isExpired(l2Entry)) {
      // Promote to L1
      this.l1Cache.set(key, l2Entry);
      return l2Entry.value as T;
    }

    // Check L3 (disk)
    const l3Entry = await this.l3Cache.get(key);
    if (l3Entry &amp;&amp; !this.isExpired(l3Entry)) {
      // Promote to L2 and L1
      this.l2Cache.set(key, l3Entry);
      this.l1Cache.set(key, l3Entry);
      return l3Entry.value as T;
    }

    return null;
  }

  /**
   * Set value in cache (writes to all levels)
   */
  async set&lt;T&gt;(
    key: string,
    value: T,
    options?: SetOptions
  ): Promise&lt;void&gt; {
    const entry: CacheEntry = {
      value,
      timestamp: Date.now(),
      ttl: options?.ttl || this.options.defaultTTL || 1000 * 60 * 60,
      size: this.estimateSize(value)
    };

    // Write to L1
    this.l1Cache.set(key, entry);

    // Write to L2
    this.l2Cache.set(key, entry);

    // Write to L3 (async, don&apos;t wait)
    this.l3Cache.set(key, entry).catch(err =&gt; {
      console.error(&apos;L3 cache write failed:&apos;, err);
    });
  }

  /**
   * Invalidate cache entry
   */
  async invalidate(key: string): Promise&lt;void&gt; {
    this.l1Cache.delete(key);
    this.l2Cache.delete(key);
    await this.l3Cache.delete(key);
  }

  /**
   * Clear entire cache
   */
  async clear(): Promise&lt;void&gt; {
    this.l1Cache.clear();
    this.l2Cache.clear();
    await this.l3Cache.clear();
  }

  /**
   * Check if entry is expired
   */
  private isExpired(entry: CacheEntry): boolean {
    return Date.now() - entry.timestamp &gt; entry.ttl;
  }

  /**
   * Estimate size of value in bytes
   */
  private estimateSize(value: any): number {
    return JSON.stringify(value).length;
  }

  /**
   * Get cache statistics
   */
  getStats(): CacheStats {
    return {
      l1Size: this.l1Cache.size,
      l2Size: this.l2Cache.size,
      l3Size: this.l3Cache.getSize(),
      totalMemoryUsage: this.calculateMemoryUsage()
    };
  }

  private calculateMemoryUsage(): number {
    let total = 0;

    for (const entry of this.l1Cache.values()) {
      total += entry.size;
    }

    return total;
  }
}

interface CacheEntry {
  value: any;
  timestamp: number;
  ttl: number;
  size: number;
}

interface CacheOptions {
  l2MaxSize?: number;
  l2TTL?: number;
  l3Path?: string;
  defaultTTL?: number;
}

interface SetOptions {
  ttl?: number;
}

interface CacheStats {
  l1Size: number;
  l2Size: number;
  l3Size: number;
  totalMemoryUsage: number;
}
</codeblock>
    <section><title>AI Response Cache</title></section>
    <codeblock outputclass="language-typescript">/**
 * Cache for AI responses with semantic key generation
 */
export class AIResponseCache {
  private cache: MultiLevelCache;
  private logger: Logger;

  constructor(cache: MultiLevelCache, logger: Logger) {
    this.cache = cache;
    this.logger = logger;
  }

  /**
   * Get cached AI response
   */
  async get(request: CompletionRequest): Promise&lt;CompletionResponse | null&gt; {
    const key = this.generateCacheKey(request);

    const cached = await this.cache.get&lt;CachedResponse&gt;(key);

    if (cached) {
      this.logger.debug(&apos;AI cache hit&apos;, { key });

      return {
        ...cached.response,
        metadata: {
          ...cached.response.metadata,
          fromCache: true,
          cacheAge: Date.now() - cached.timestamp
        }
      };
    }

    this.logger.debug(&apos;AI cache miss&apos;, { key });
    return null;
  }

  /**
   * Cache AI response
   */
  async set(
    request: CompletionRequest,
    response: CompletionResponse,
    ttl?: number
  ): Promise&lt;void&gt; {
    const key = this.generateCacheKey(request);

    await this.cache.set(key, {
      request,
      response,
      timestamp: Date.now()
    }, { ttl });

    this.logger.debug(&apos;AI response cached&apos;, { key });
  }

  /**
   * Generate cache key from request
   * Uses semantic hashing to match similar requests
   */
  private generateCacheKey(request: CompletionRequest): string {
    // Normalize request for consistent caching
    const normalized = {
      model: request.model,
      messages: request.messages.map(m =&gt; ({
        role: m.role,
        content: this.normalizeContent(m.content)
      })),
      temperature: request.temperature || 0.7,
      maxTokens: request.maxTokens
    };

    // Hash normalized request
    const hash = crypto
      .createHash(&apos;sha256&apos;)
      .update(JSON.stringify(normalized))
      .digest(&apos;hex&apos;);

    return `ai:${hash}`;
  }

  /**
   * Normalize content to improve cache hit rate
   */
  private normalizeContent(content: string): string {
    return content
      .trim()
      .replace(/\s+/g, &apos; &apos;) // Normalize whitespace
      .toLowerCase();
  }

  /**
   * Get cache hit rate
   */
  async getHitRate(): Promise&lt;number&gt; {
    // Implement hit rate tracking
    // This would require tracking hits and misses
    return 0.75; // Placeholder
  }
}

interface CachedResponse {
  request: CompletionRequest;
  response: CompletionResponse;
  timestamp: number;
}
</codeblock>
    <section><title>Tool Result Cache</title></section>
    <codeblock outputclass="language-typescript">/**
 * Cache for tool execution results
 */
export class ToolResultCache {
  private cache: MultiLevelCache;

  constructor(cache: MultiLevelCache) {
    this.cache = cache;
  }

  /**
   * Get cached tool result
   */
  async get(
    toolName: string,
    params: Record&lt;string, any&gt;
  ): Promise&lt;ToolResult | null&gt; {
    const key = this.generateKey(toolName, params);
    return this.cache.get&lt;ToolResult&gt;(key);
  }

  /**
   * Cache tool result
   */
  async set(
    toolName: string,
    params: Record&lt;string, any&gt;,
    result: ToolResult,
    ttl?: number
  ): Promise&lt;void&gt; {
    const key = this.generateKey(toolName, params);
    await this.cache.set(key, result, { ttl });
  }

  /**
   * Invalidate cache for specific file
   */
  async invalidateFile(filePath: string): Promise&lt;void&gt; {
    // Invalidate all tool results related to this file
    const patterns = [
      `tool:read_file:${filePath}`,
      `tool:search_code:*${filePath}*`,
      `tool:analyze_code:${filePath}`
    ];

    for (const pattern of patterns) {
      await this.cache.invalidate(pattern);
    }
  }

  /**
   * Generate cache key
   */
  private generateKey(
    toolName: string,
    params: Record&lt;string, any&gt;
  ): string {
    // Sort params for consistent key generation
    const sortedParams = Object.keys(params)
      .sort()
      .reduce((acc, key) =&gt; {
        acc[key] = params[key];
        return acc;
      }, {} as Record&lt;string, any&gt;);

    const paramsHash = crypto
      .createHash(&apos;sha256&apos;)
      .update(JSON.stringify(sortedParams))
      .digest(&apos;hex&apos;)
      .substring(0, 16);

    return `tool:${toolName}:${paramsHash}`;
  }
}
</codeblock>
    <section><title>Cache-Aware Service</title></section>
    <codeblock outputclass="language-typescript">/**
 * AI service with intelligent caching
 */
export class CachedAIService {
  private provider: AIProvider;
  private aiCache: AIResponseCache;
  private toolCache: ToolResultCache;
  private logger: Logger;

  constructor(
    provider: AIProvider,
    cache: MultiLevelCache,
    logger: Logger
  ) {
    this.provider = provider;
    this.aiCache = new AIResponseCache(cache, logger);
    this.toolCache = new ToolResultCache(cache);
    this.logger = logger;
  }

  /**
   * Complete with caching
   */
  async complete(request: CompletionRequest): Promise&lt;CompletionResponse&gt; {
    // Check cache first
    const cached = await this.aiCache.get(request);

    if (cached) {
      return cached;
    }

    // Cache miss - call provider
    const startTime = performance.now();

    const response = await this.provider.complete(request);

    const duration = performance.now() - startTime;

    // Cache response (TTL based on temperature)
    const ttl = this.calculateTTL(request.temperature || 0.7);
    await this.aiCache.set(request, response, ttl);

    this.logger.info(&apos;AI completion&apos;, {
      cached: false,
      duration,
      tokens: response.usage?.totalTokens
    });

    return response;
  }

  /**
   * Execute tool with caching
   */
  async executeTool(
    tool: Tool,
    params: Record&lt;string, any&gt;,
    context: ToolContext
  ): Promise&lt;ToolResult&gt; {
    // Skip cache for non-cacheable tools
    if (!tool.cacheable) {
      return tool.execute(params, context);
    }

    // Check cache
    const cached = await this.toolCache.get(tool.name, params);

    if (cached) {
      this.logger.debug(&apos;Tool cache hit&apos;, { tool: tool.name });
      return cached;
    }

    // Cache miss - execute tool
    const result = await tool.execute(params, context);

    // Cache result (TTL based on tool type)
    const ttl = this.getToolCacheTTL(tool.name);
    await this.toolCache.set(tool.name, params, result, ttl);

    return result;
  }

  /**
   * Calculate cache TTL based on temperature
   * Higher temperature = shorter TTL (more variation)
   */
  private calculateTTL(temperature: number): number {
    const baseTTL = 1000 * 60 * 60; // 1 hour

    if (temperature &lt; 0.3) {
      return baseTTL * 24; // 24 hours (very deterministic)
    } else if (temperature &lt; 0.7) {
      return baseTTL * 4; // 4 hours
    } else {
      return baseTTL; // 1 hour (more random)
    }
  }

  /**
   * Get cache TTL for tool
   */
  private getToolCacheTTL(toolName: string): number {
    const ttls: Record&lt;string, number&gt; = {
      read_file: 1000 * 60 * 5,      // 5 minutes (files change)
      search_code: 1000 * 60 * 10,   // 10 minutes
      git_status: 1000 * 60 * 1,     // 1 minute (changes frequently)
      analyze_code: 1000 * 60 * 30   // 30 minutes (expensive)
    };

    return ttls[toolName] || 1000 * 60 * 5; // Default 5 minutes
  }

  /**
   * Get cache statistics
   */
  async getCacheStats(): Promise&lt;{
    hitRate: number;
    size: number;
    memoryUsage: number;
  }&gt; {
    const hitRate = await this.aiCache.getHitRate();
    // Would implement full stats tracking

    return {
      hitRate,
      size: 0,
      memoryUsage: 0
    };
  }
}
</codeblock>
    <section><title>11.3 Parallel Execution Optimization</title></section>
    <p>Execute independent operations in parallel instead of sequentially.</p>
    <section><title>Parallel Executor</title></section>
    <codeblock outputclass="language-typescript">/**
 * Executes operations in parallel with concurrency control
 */
export class ParallelExecutor {
  constructor(
    private maxConcurrency: number = 10,
    private logger: Logger
  ) {}

  /**
   * Execute operations in parallel with concurrency limit
   */
  async executeAll&lt;T&gt;(
    operations: Array&lt;() =&gt; Promise&lt;T&gt;&gt;,
    options?: ExecutionOptions
  ): Promise&lt;T[]&gt; {
    const results: T[] = [];
    const executing: Promise&lt;void&gt;[] = [];

    const startTime = performance.now();

    for (let i = 0; i &lt; operations.length; i++) {
      const operation = operations[i];

      // Execute operation
      const promise = operation()
        .then(result =&gt; {
          results[i] = result;
        })
        .catch(error =&gt; {
          if (options?.failFast) {
            throw error;
          }

          this.logger.error(&apos;Parallel execution error&apos;, { index: i, error });
          results[i] = error;
        });

      executing.push(promise);

      // Wait if we&apos;ve hit concurrency limit
      if (executing.length &gt;= this.maxConcurrency) {
        await Promise.race(executing);

        // Remove completed promises
        const completed = executing.filter(p =&gt; {
          // Check if promise is settled (no good way in JS, so we track separately)
          return false; // Placeholder
        });

        completed.forEach(p =&gt; {
          const index = executing.indexOf(p);
          if (index &gt; -1) executing.splice(index, 1);
        });
      }
    }

    // Wait for remaining operations
    await Promise.all(executing);

    const duration = performance.now() - startTime;

    this.logger.info(&apos;Parallel execution completed&apos;, {
      operations: operations.length,
      duration,
      throughput: operations.length / (duration / 1000)
    });

    return results;
  }

  /**
   * Execute operations in batches
   */
  async executeBatches&lt;T&gt;(
    operations: Array&lt;() =&gt; Promise&lt;T&gt;&gt;,
    batchSize: number = this.maxConcurrency
  ): Promise&lt;T[]&gt; {
    const results: T[] = [];

    for (let i = 0; i &lt; operations.length; i += batchSize) {
      const batch = operations.slice(i, i + batchSize);

      this.logger.debug(&apos;Executing batch&apos;, {
        batch: Math.floor(i / batchSize) + 1,
        size: batch.length
      });

      const batchResults = await Promise.all(
        batch.map(op =&gt; op())
      );

      results.push(...batchResults);
    }

    return results;
  }

  /**
   * Map operation over array in parallel
   */
  async map&lt;T, R&gt;(
    items: T[],
    fn: (item: T, index: number) =&gt; Promise&lt;R&gt;,
    concurrency: number = this.maxConcurrency
  ): Promise&lt;R[]&gt; {
    const operations = items.map((item, index) =&gt; () =&gt; fn(item, index));

    return this.executeAll(operations);
  }
}

interface ExecutionOptions {
  failFast?: boolean; // Stop on first error
  timeout?: number;   // Timeout per operation
}
</codeblock>
    <section><title>Parallel Tool Execution</title></section>
    <codeblock outputclass="language-typescript">/**
 * Optimized tool orchestrator with parallel execution
 */
export class ParallelToolOrchestrator extends ToolOrchestrator {
  private executor: ParallelExecutor;

  constructor(
    aiProvider: AIProvider,
    options: OrchestratorOptions
  ) {
    super(aiProvider, options);
    this.executor = new ParallelExecutor(
      options.maxParallelTools || 5,
      options.logger
    );
  }

  /**
   * Execute tools in parallel based on dependency graph
   */
  async executePlan(plan: ExecutionPlan): Promise&lt;ExecutionResult&gt; {
    const graph = this.buildDependencyGraph(plan.toolCalls);

    // Get execution levels (tools that can run in parallel)
    const levels = graph.getExecutionLevels();

    const results: ToolResult[] = [];
    const resultMap = new Map&lt;string, ToolResult&gt;();

    // Execute each level in parallel
    for (let levelIndex = 0; levelIndex &lt; levels.length; levelIndex++) {
      const level = levels[levelIndex];

      this.logger.info(&apos;Executing tool level&apos;, {
        level: levelIndex + 1,
        tools: level.length
      });

      // Execute all tools in this level in parallel
      const levelResults = await this.executor.map(
        level,
        async (toolId) =&gt; {
          const toolCall = plan.toolCalls.find(tc =&gt; tc.id === toolId)!;

          // Resolve parameters (may reference previous tool results)
          const params = this.resolveParameters(
            toolCall.parameters,
            resultMap
          );

          // Execute tool
          const tool = this.registry.get(toolCall.tool);
          if (!tool) {
            throw new Error(`Tool not found: ${toolCall.tool}`);
          }

          const result = await this.cachedExecute(tool, params);

          // Store result for dependent tools
          resultMap.set(toolId, result);

          return result;
        }
      );

      results.push(...levelResults);
    }

    return {
      success: true,
      toolResults: results,
      metadata: {
        levels: levels.length,
        parallelism: levels.map(l =&gt; l.length)
      }
    };
  }

  /**
   * Cached tool execution
   */
  private async cachedExecute(
    tool: Tool,
    params: Record&lt;string, any&gt;
  ): Promise&lt;ToolResult&gt; {
    // Use cache if available
    if (this.cache &amp;&amp; tool.cacheable) {
      const cached = await this.cache.get(tool.name, params);
      if (cached) {
        return cached;
      }
    }

    const result = await tool.execute(params, this.context);

    // Cache result
    if (this.cache &amp;&amp; tool.cacheable) {
      await this.cache.set(tool.name, params, result);
    }

    return result;
  }
}
</codeblock>
    <section><title>Parallel File Processing</title></section>
    <codeblock outputclass="language-typescript">/**
 * Process multiple files in parallel
 */
export class ParallelFileProcessor {
  constructor(
    private ai: AIProvider,
    private executor: ParallelExecutor,
    private logger: Logger
  ) {}

  /**
   * Analyze multiple files in parallel
   */
  async analyzeFiles(
    filePaths: string[],
    options?: AnalysisOptions
  ): Promise&lt;Map&lt;string, Analysis&gt;&gt; {
    this.logger.info(&apos;Analyzing files in parallel&apos;, {
      count: filePaths.length
    });

    const startTime = performance.now();

    // Execute in parallel
    const analyses = await this.executor.map(
      filePaths,
      async (filePath) =&gt; {
        // Read file
        const content = await fs.readFile(filePath, &apos;utf-8&apos;);

        // Analyze with AI (cached)
        const analysis = await this.analyzeContent(content, filePath);

        return { filePath, analysis };
      },
      options?.concurrency || 5
    );

    const duration = performance.now() - startTime;

    this.logger.info(&apos;File analysis completed&apos;, {
      count: filePaths.length,
      duration,
      avgDuration: duration / filePaths.length
    });

    // Convert to map
    const resultMap = new Map&lt;string, Analysis&gt;();
    analyses.forEach(({ filePath, analysis }) =&gt; {
      resultMap.set(filePath, analysis);
    });

    return resultMap;
  }

  /**
   * Analyze file content
   */
  private async analyzeContent(
    content: string,
    filePath: string
  ): Promise&lt;Analysis&gt; {
    const response = await this.ai.complete({
      messages: [{
        role: MessageRole.USER,
        content: `Analyze this ${path.extname(filePath)} file:\n\n${content}`
      }],
      temperature: 0.3
    });

    return this.parseAnalysis(response.content);
  }

  private parseAnalysis(content: string): Analysis {
    // Parse AI response into structured analysis
    return {
      summary: content,
      complexity: 0,
      issues: []
    };
  }
}

interface AnalysisOptions {
  concurrency?: number;
}

interface Analysis {
  summary: string;
  complexity: number;
  issues: string[];
}
</codeblock>
    <section><title>11.4 Lazy Loading Patterns</title></section>
    <p>Load modules and data only when needed to reduce startup time and memory usage.</p>
    <section><title>Lazy Module Loader</title></section>
    <codeblock outputclass="language-typescript">/**
 * Lazy loads modules on demand
 */
export class LazyModuleLoader&lt;T&gt; {
  private instance?: T;
  private loading?: Promise&lt;T&gt;;

  constructor(
    private loader: () =&gt; Promise&lt;T&gt;,
    private logger?: Logger
  ) {}

  /**
   * Get module instance (loads if needed)
   */
  async get(): Promise&lt;T&gt; {
    // Return cached instance
    if (this.instance) {
      return this.instance;
    }

    // Return in-flight loading promise
    if (this.loading) {
      return this.loading;
    }

    // Start loading
    this.logger?.debug(&apos;Lazy loading module&apos;);

    const startTime = performance.now();

    this.loading = this.loader();

    try {
      this.instance = await this.loading;

      const duration = performance.now() - startTime;

      this.logger?.debug(&apos;Module loaded&apos;, { duration });

      return this.instance;

    } finally {
      this.loading = undefined;
    }
  }

  /**
   * Check if module is loaded
   */
  isLoaded(): boolean {
    return this.instance !== undefined;
  }

  /**
   * Unload module (for memory management)
   */
  unload(): void {
    this.instance = undefined;
  }
}
</codeblock>
    <section><title>Lazy Command Registry</title></section>
    <codeblock outputclass="language-typescript">/**
 * Command registry with lazy loading
 */
export class LazyCommandRegistry {
  private loaders: Map&lt;string, LazyModuleLoader&lt;RoutableCommand&gt;&gt; = new Map();
  private logger: Logger;

  constructor(logger: Logger) {
    this.logger = logger;
  }

  /**
   * Register command with lazy loader
   */
  register(
    name: string,
    loader: () =&gt; Promise&lt;new () =&gt; RoutableCommand&gt;,
    dependencies?: any
  ): void {
    const moduleLoader = new LazyModuleLoader(
      async () =&gt; {
        const CommandClass = await loader();
        return new CommandClass(dependencies);
      },
      this.logger
    );

    this.loaders.set(name, moduleLoader);
  }

  /**
   * Get command (loads if needed)
   */
  async get(name: string): Promise&lt;RoutableCommand | null&gt; {
    const loader = this.loaders.get(name);

    if (!loader) {
      return null;
    }

    return loader.get();
  }

  /**
   * Preload commonly used commands
   */
  async preload(commandNames: string[]): Promise&lt;void&gt; {
    this.logger.info(&apos;Preloading commands&apos;, { commands: commandNames });

    await Promise.all(
      commandNames.map(name =&gt; this.get(name))
    );
  }

  /**
   * Get loading statistics
   */
  getStats(): {
    total: number;
    loaded: number;
    notLoaded: number;
  } {
    let loaded = 0;

    for (const loader of this.loaders.values()) {
      if (loader.isLoaded()) {
        loaded++;
      }
    }

    return {
      total: this.loaders.size,
      loaded,
      notLoaded: this.loaders.size - loaded
    };
  }
}
</codeblock>
    <section><title>Lazy Data Loading</title></section>
    <codeblock outputclass="language-typescript">/**
 * Lazy loads large datasets
 */
export class LazyDataset&lt;T&gt; {
  private data?: T[];
  private loading?: Promise&lt;T[]&gt;;

  constructor(
    private loader: () =&gt; Promise&lt;T[]&gt;,
    private pageSize: number = 100
  ) {}

  /**
   * Get page of data
   */
  async getPage(page: number): Promise&lt;T[]&gt; {
    const allData = await this.getAll();

    const start = page * this.pageSize;
    const end = start + this.pageSize;

    return allData.slice(start, end);
  }

  /**
   * Get all data (loads if needed)
   */
  async getAll(): Promise&lt;T[]&gt; {
    if (this.data) {
      return this.data;
    }

    if (this.loading) {
      return this.loading;
    }

    this.loading = this.loader();
    this.data = await this.loading;
    this.loading = undefined;

    return this.data;
  }

  /**
   * Unload data to free memory
   */
  unload(): void {
    this.data = undefined;
  }
}
</codeblock>
    <section><title>11.5 Memory Management</title></section>
    <p>Manage memory carefully to prevent leaks and reduce usage.</p>
    <section><title>Memory-Aware Conversation Manager</title></section>
    <codeblock outputclass="language-typescript">/**
 * Conversation manager with memory limits
 */
export class MemoryAwareConversationManager extends ConversationManager {
  private maxMemoryBytes: number;

  constructor(
    aiProvider: AIProvider,
    options: ConversationOptions &amp; { maxMemoryMB?: number }
  ) {
    super(aiProvider, options);
    this.maxMemoryBytes = (options.maxMemoryMB || 100) * 1024 * 1024;
  }

  /**
   * Add message with memory check
   */
  override addMessage(message: Message): void {
    super.addMessage(message);

    // Check memory usage
    const currentMemory = this.estimateMemoryUsage();

    if (currentMemory &gt; this.maxMemoryBytes) {
      this.trimMessages();
    }
  }

  /**
   * Estimate memory usage of conversation
   */
  private estimateMemoryUsage(): number {
    let bytes = 0;

    for (const message of this.messages) {
      // Rough estimate: 2 bytes per character (UTF-16)
      bytes += message.content.length * 2;

      // Add overhead for object structure
      bytes += 100;
    }

    return bytes;
  }

  /**
   * Trim old messages to reduce memory
   */
  private trimMessages(): void {
    const systemMessages = this.messages.filter(
      m =&gt; m.role === MessageRole.SYSTEM
    );

    const nonSystemMessages = this.messages.filter(
      m =&gt; m.role !== MessageRole.SYSTEM
    );

    // Keep system messages + recent 50% of conversation
    const keepCount = Math.floor(nonSystemMessages.length / 2);
    const keptMessages = nonSystemMessages.slice(-keepCount);

    this.messages = [...systemMessages, ...keptMessages];

    this.logger.info(&apos;Trimmed conversation for memory&apos;, {
      removed: nonSystemMessages.length - keptMessages.length,
      remaining: this.messages.length
    });
  }

  /**
   * Get memory statistics
   */
  getMemoryStats(): {
    messages: number;
    bytes: number;
    megabytes: number;
  } {
    const bytes = this.estimateMemoryUsage();

    return {
      messages: this.messages.length,
      bytes,
      megabytes: bytes / 1024 / 1024
    };
  }
}
</codeblock>
    <section><title>Object Pooling</title></section>
    <codeblock outputclass="language-typescript">/**
 * Object pool to reduce allocations
 */
export class ObjectPool&lt;T&gt; {
  private available: T[] = [];
  private inUse = new Set&lt;T&gt;();

  constructor(
    private factory: () =&gt; T,
    private reset: (obj: T) =&gt; void,
    private initialSize: number = 10
  ) {
    // Pre-allocate objects
    for (let i = 0; i &lt; initialSize; i++) {
      this.available.push(this.factory());
    }
  }

  /**
   * Acquire object from pool
   */
  acquire(): T {
    let obj: T;

    if (this.available.length &gt; 0) {
      obj = this.available.pop()!;
    } else {
      // Pool exhausted - create new object
      obj = this.factory();
    }

    this.inUse.add(obj);

    return obj;
  }

  /**
   * Release object back to pool
   */
  release(obj: T): void {
    if (!this.inUse.has(obj)) {
      throw new Error(&apos;Object not in use&apos;);
    }

    this.inUse.delete(obj);

    // Reset object state
    this.reset(obj);

    this.available.push(obj);
  }

  /**
   * Get pool statistics
   */
  getStats(): {
    available: number;
    inUse: number;
    total: number;
  } {
    return {
      available: this.available.length,
      inUse: this.inUse.size,
      total: this.available.length + this.inUse.size
    };
  }
}

/**
 * Example: Pool for message objects
 */
const messagePool = new ObjectPool&lt;Message&gt;(
  () =&gt; ({
    id: &apos;&apos;,
    role: MessageRole.USER,
    content: &apos;&apos;,
    timestamp: new Date(),
    metadata: {}
  }),
  (msg) =&gt; {
    msg.id = &apos;&apos;;
    msg.role = MessageRole.USER;
    msg.content = &apos;&apos;;
    msg.timestamp = new Date();
    msg.metadata = {};
  },
  100 // Pre-allocate 100 messages
);
</codeblock>
    <section><title>Memory Profiling</title></section>
    <codeblock outputclass="language-typescript">/**
 * Memory profiler for detecting leaks
 */
export class MemoryProfiler {
  private snapshots: MemorySnapshot[] = [];

  /**
   * Take memory snapshot
   */
  snapshot(label: string): MemorySnapshot {
    const usage = process.memoryUsage();

    const snapshot: MemorySnapshot = {
      label,
      timestamp: Date.now(),
      heapUsed: usage.heapUsed,
      heapTotal: usage.heapTotal,
      external: usage.external,
      rss: usage.rss
    };

    this.snapshots.push(snapshot);

    return snapshot;
  }

  /**
   * Compare two snapshots
   */
  compare(label1: string, label2: string): MemoryComparison {
    const snap1 = this.snapshots.find(s =&gt; s.label === label1);
    const snap2 = this.snapshots.find(s =&gt; s.label === label2);

    if (!snap1 || !snap2) {
      throw new Error(&apos;Snapshot not found&apos;);
    }

    return {
      heapUsedDelta: snap2.heapUsed - snap1.heapUsed,
      heapTotalDelta: snap2.heapTotal - snap1.heapTotal,
      externalDelta: snap2.external - snap1.external,
      rssDelta: snap2.rss - snap1.rss,
      durationMs: snap2.timestamp - snap1.timestamp
    };
  }

  /**
   * Detect memory leak
   */
  detectLeak(threshold: number = 10 * 1024 * 1024): boolean {
    if (this.snapshots.length &lt; 2) {
      return false;
    }

    const first = this.snapshots[0];
    const last = this.snapshots[this.snapshots.length - 1];

    const delta = last.heapUsed - first.heapUsed;

    return delta &gt; threshold;
  }

  /**
   * Get memory trend
   */
  getTrend(): &apos;increasing&apos; | &apos;stable&apos; | &apos;decreasing&apos; {
    if (this.snapshots.length &lt; 3) {
      return &apos;stable&apos;;
    }

    const recent = this.snapshots.slice(-5);

    const deltas = recent.slice(1).map((snap, i) =&gt; {
      return snap.heapUsed - recent[i].heapUsed;
    });

    const avgDelta = deltas.reduce((a, b) =&gt; a + b, 0) / deltas.length;

    if (avgDelta &gt; 1024 * 1024) {
      return &apos;increasing&apos;;
    } else if (avgDelta &lt; -1024 * 1024) {
      return &apos;decreasing&apos;;
    } else {
      return &apos;stable&apos;;
    }
  }
}

interface MemorySnapshot {
  label: string;
  timestamp: number;
  heapUsed: number;
  heapTotal: number;
  external: number;
  rss: number;
}

interface MemoryComparison {
  heapUsedDelta: number;
  heapTotalDelta: number;
  externalDelta: number;
  rssDelta: number;
  durationMs: number;
}
</codeblock>
    <section><title>11.6 Connection Pooling</title></section>
    <p>Reuse HTTP connections to reduce overhead.</p>
    <section><title>HTTP Connection Pool</title></section>
    <codeblock outputclass="language-typescript">import http from &apos;http&apos;;
import https from &apos;https&apos;;

/**
 * HTTP agent with connection pooling
 */
export class ConnectionPool {
  private httpAgent: http.Agent;
  private httpsAgent: https.Agent;

  constructor(options: PoolOptions = {}) {
    this.httpAgent = new http.Agent({
      keepAlive: true,
      keepAliveMsecs: options.keepAliveMsecs || 1000,
      maxSockets: options.maxSockets || 50,
      maxFreeSockets: options.maxFreeSockets || 10,
      timeout: options.timeout || 60000
    });

    this.httpsAgent = new https.Agent({
      keepAlive: true,
      keepAliveMsecs: options.keepAliveMsecs || 1000,
      maxSockets: options.maxSockets || 50,
      maxFreeSockets: options.maxFreeSockets || 10,
      timeout: options.timeout || 60000
    });
  }

  /**
   * Get HTTP agent
   */
  getHttpAgent(): http.Agent {
    return this.httpAgent;
  }

  /**
   * Get HTTPS agent
   */
  getHttpsAgent(): https.Agent {
    return this.httpsAgent;
  }

  /**
   * Get agent stats
   */
  getStats(): PoolStats {
    return {
      http: this.getAgentStats(this.httpAgent),
      https: this.getAgentStats(this.httpsAgent)
    };
  }

  private getAgentStats(agent: http.Agent): AgentStats {
    return {
      sockets: Object.keys(agent.sockets).length,
      freeSockets: Object.keys(agent.freeSockets || {}).length,
      requests: Object.keys(agent.requests || {}).length
    };
  }

  /**
   * Destroy all connections
   */
  destroy(): void {
    this.httpAgent.destroy();
    this.httpsAgent.destroy();
  }
}

interface PoolOptions {
  keepAliveMsecs?: number;
  maxSockets?: number;
  maxFreeSockets?: number;
  timeout?: number;
}

interface PoolStats {
  http: AgentStats;
  https: AgentStats;
}

interface AgentStats {
  sockets: number;
  freeSockets: number;
  requests: number;
}
</codeblock>
    <section><title>AI Provider with Connection Pooling</title></section>
    <codeblock outputclass="language-typescript">/**
 * AI provider using connection pool
 */
export class PooledAIProvider implements AIProvider {
  private pool: ConnectionPool;
  private baseURL: string;

  constructor(
    options: ProviderOptions,
    pool?: ConnectionPool
  ) {
    this.pool = pool || new ConnectionPool();
    this.baseURL = options.baseURL;
  }

  async complete(request: CompletionRequest): Promise&lt;CompletionResponse&gt; {
    const response = await fetch(this.baseURL + &apos;/completions&apos;, {
      method: &apos;POST&apos;,
      headers: {
        &apos;Content-Type&apos;: &apos;application/json&apos;
      },
      body: JSON.stringify(request),
      // Use connection pool
      agent: this.pool.getHttpsAgent()
    });

    return response.json();
  }

  // ... other methods
}
</codeblock>
    <section><title>11.7 Response Streaming</title></section>
    <p>Stream responses to reduce perceived latency.</p>
    <section><title>Streaming Response Handler</title></section>
    <codeblock outputclass="language-typescript">/**
 * Handles streaming responses for lower perceived latency
 */
export class StreamingResponseHandler {
  async handleStream(
    stream: AsyncIterableIterator&lt;StreamEvent&gt;,
    onToken: (token: string) =&gt; void,
    onComplete: (fullResponse: string) =&gt; void
  ): Promise&lt;void&gt; {
    let fullResponse = &apos;&apos;;
    let firstTokenTime: number | null = null;

    const startTime = performance.now();

    for await (const event of stream) {
      if (event.type === StreamEventType.CONTENT) {
        // Record first token latency
        if (!firstTokenTime) {
          firstTokenTime = performance.now() - startTime;
          console.log(`First token: ${firstTokenTime.toFixed(2)}ms`);
        }

        fullResponse += event.content;
        onToken(event.content);
      } else if (event.type === StreamEventType.DONE) {
        break;
      }
    }

    const totalTime = performance.now() - startTime;

    console.log(`Total time: ${totalTime.toFixed(2)}ms`);
    console.log(`Tokens/second: ${(fullResponse.length / (totalTime / 1000)).toFixed(2)}`);

    onComplete(fullResponse);
  }
}

/**
 * Example usage
 */
async function streamExample() {
  const provider = new AnthropicProvider({ apiKey: &apos;...&apos; });

  const stream = provider.stream({
    messages: [{
      role: MessageRole.USER,
      content: &apos;Write a long essay about AI&apos;
    }]
  });

  const handler = new StreamingResponseHandler();

  await handler.handleStream(
    stream,
    (token) =&gt; {
      // Display token immediately
      process.stdout.write(token);
    },
    (fullResponse) =&gt; {
      console.log(&apos;\n\nComplete!&apos;);
      console.log(`Total length: ${fullResponse.length} characters`);
    }
  );
}
</codeblock>
    <section><title>11.8 Profiling and Benchmarking</title></section>
    <p>Measure performance to identify bottlenecks and track improvements.</p>
    <section><title>Performance Profiler</title></section>
    <codeblock outputclass="language-typescript">/**
 * Profiles code execution to find bottlenecks
 */
export class PerformanceProfiler {
  private measurements: Map&lt;string, Measurement[]&gt; = new Map();

  /**
   * Start timing an operation
   */
  start(label: string): () =&gt; void {
    const startTime = performance.now();

    return () =&gt; {
      const duration = performance.now() - startTime;

      if (!this.measurements.has(label)) {
        this.measurements.set(label, []);
      }

      this.measurements.get(label)!.push({
        duration,
        timestamp: Date.now()
      });
    };
  }

  /**
   * Measure async operation
   */
  async measure&lt;T&gt;(
    label: string,
    fn: () =&gt; Promise&lt;T&gt;
  ): Promise&lt;T&gt; {
    const end = this.start(label);

    try {
      return await fn();
    } finally {
      end();
    }
  }

  /**
   * Get statistics for label
   */
  getStats(label: string): ProfileStats | null {
    const measurements = this.measurements.get(label);

    if (!measurements || measurements.length === 0) {
      return null;
    }

    const durations = measurements.map(m =&gt; m.duration).sort((a, b) =&gt; a - b);

    return {
      count: measurements.length,
      mean: durations.reduce((a, b) =&gt; a + b) / measurements.length,
      median: durations[Math.floor(durations.length / 2)],
      min: durations[0],
      max: durations[durations.length - 1],
      p95: durations[Math.floor(durations.length * 0.95)],
      p99: durations[Math.floor(durations.length * 0.99)]
    };
  }

  /**
   * Get report of all measurements
   */
  getReport(): ProfileReport {
    const report: ProfileReport = {};

    for (const [label, measurements] of this.measurements.entries()) {
      const stats = this.getStats(label);
      if (stats) {
        report[label] = stats;
      }
    }

    return report;
  }

  /**
   * Print report to console
   */
  printReport(): void {
    console.log(&apos;\n📊 Performance Profile Report\n&apos;);
    console.table(this.getReport());
  }

  /**
   * Clear all measurements
   */
  clear(): void {
    this.measurements.clear();
  }
}

interface Measurement {
  duration: number;
  timestamp: number;
}

interface ProfileStats {
  count: number;
  mean: number;
  median: number;
  min: number;
  max: number;
  p95: number;
  p99: number;
}

type ProfileReport = Record&lt;string, ProfileStats&gt;;
</codeblock>
    <section><title>Benchmarking Suite</title></section>
    <codeblock outputclass="language-typescript">/**
 * Comprehensive benchmarking suite
 */
export class BenchmarkSuite {
  private benchmarks: Benchmark[] = [];

  /**
   * Add benchmark
   */
  add(name: string, fn: () =&gt; Promise&lt;void&gt;): this {
    this.benchmarks.push({ name, fn });
    return this;
  }

  /**
   * Run all benchmarks
   */
  async run(iterations: number = 100): Promise&lt;BenchmarkResults&gt; {
    const results: BenchmarkResults = {};

    for (const benchmark of this.benchmarks) {
      console.log(`Running: ${benchmark.name}`);

      const durations: number[] = [];

      for (let i = 0; i &lt; iterations; i++) {
        const start = performance.now();
        await benchmark.fn();
        durations.push(performance.now() - start);
      }

      results[benchmark.name] = this.calculateStats(durations);
    }

    return results;
  }

  /**
   * Compare benchmarks
   */
  compare(
    baseline: string,
    comparison: string,
    results: BenchmarkResults
  ): Comparison {
    const baselineStats = results[baseline];
    const comparisonStats = results[comparison];

    if (!baselineStats || !comparisonStats) {
      throw new Error(&apos;Benchmark not found&apos;);
    }

    const improvement = (baselineStats.mean - comparisonStats.mean) / baselineStats.mean;

    return {
      baseline: baseline,
      comparison: comparison,
      baselineMean: baselineStats.mean,
      comparisonMean: comparisonStats.mean,
      improvement: improvement * 100,
      faster: improvement &gt; 0
    };
  }

  private calculateStats(durations: number[]): ProfileStats {
    const sorted = durations.sort((a, b) =&gt; a - b);

    return {
      count: durations.length,
      mean: durations.reduce((a, b) =&gt; a + b) / durations.length,
      median: sorted[Math.floor(sorted.length / 2)],
      min: sorted[0],
      max: sorted[sorted.length - 1],
      p95: sorted[Math.floor(sorted.length * 0.95)],
      p99: sorted[Math.floor(sorted.length * 0.99)]
    };
  }
}

interface Benchmark {
  name: string;
  fn: () =&gt; Promise&lt;void&gt;;
}

type BenchmarkResults = Record&lt;string, ProfileStats&gt;;

interface Comparison {
  baseline: string;
  comparison: string;
  baselineMean: number;
  comparisonMean: number;
  improvement: number; // percentage
  faster: boolean;
}
</codeblock>
    <section><title>Example Benchmarks</title></section>
    <codeblock outputclass="language-typescript">// Create benchmark suite
const suite = new BenchmarkSuite();

// Add benchmarks
suite
  .add(&apos;sequential-file-read&apos;, async () =&gt; {
    for (let i = 0; i &lt; 10; i++) {
      await fs.readFile(`file${i}.txt`);
    }
  })
  .add(&apos;parallel-file-read&apos;, async () =&gt; {
    await Promise.all(
      Array.from({ length: 10 }, (_, i) =&gt;
        fs.readFile(`file${i}.txt`)
      )
    );
  })
  .add(&apos;cached-ai-call&apos;, async () =&gt; {
    await cachedAI.complete(request);
  })
  .add(&apos;uncached-ai-call&apos;, async () =&gt; {
    await uncachedAI.complete(request);
  });

// Run benchmarks
const results = await suite.run(100);

// Compare
const comparison = suite.compare(
  &apos;sequential-file-read&apos;,
  &apos;parallel-file-read&apos;,
  results
);

console.log(`Parallel is ${comparison.improvement.toFixed(2)}% faster`);
</codeblock>
    <section><title>Exercises</title></section>
    <section><title>Exercise 1: Implement Adaptive Caching</title></section>
    <p><b>Goal:</b> Build a cache that adapts TTL based on access patterns.</p>
    <p><b>Requirements:</b>
1. Track access frequency per key
2. Increase TTL for frequently accessed items
3. Decrease TTL for rarely accessed items
4. Implement LFU (Least Frequently Used) eviction</p>
    <p><b>Starter Code:</b></p>
    <codeblock outputclass="language-typescript">export class AdaptiveCache {
  async get(key: string): Promise&lt;any&gt; {
    // TODO: Track access, adjust TTL
  }

  async set(key: string, value: any): Promise&lt;void&gt; {
    // TODO: Set with adaptive TTL
  }
}
</codeblock>
    <section><title>Exercise 2: Build a Query Optimizer</title></section>
    <p><b>Goal:</b> Optimize multiple queries by deduplication and batching.</p>
    <p><b>Requirements:</b>
1. Deduplicate identical concurrent requests
2. Batch similar requests together
3. Cache results for deduped requests
4. Measure improvement</p>
    <p><b>Hints:</b>
- Use request fingerprinting for deduplication
- Implement request queue with timeout
- Batch requests to same provider</p>
    <section><title>Exercise 3: Memory Leak Detection</title></section>
    <p><b>Goal:</b> Create automated memory leak detection system.</p>
    <p><b>Requirements:</b>
1. Take periodic memory snapshots
2. Analyze memory growth trends
3. Detect potential leaks
4. Generate alerts
5. Identify leak sources</p>
    <section><title>Summary</title></section>
    <p>In this chapter, you optimized your AI assistant for production performance.</p>
    <section><title>Key Concepts</title></section>
    <ol>
      <li>
        <b>Intelligent Caching</b>
        - Multi-level cache with semantic keys (70%+ hit rate)
      </li>
      <li>
        <b>Parallel Execution</b>
        - Execute independent operations concurrently (3-5x faster)
      </li>
      <li>
        <b>Lazy Loading</b>
        - Load modules on demand (50% faster startup)
      </li>
      <li>
        <b>Memory Management</b>
        - Prevent leaks, reduce usage (60% less memory)
      </li>
      <li>
        <b>Connection Pooling</b>
        - Reuse HTTP connections (lower latency)
      </li>
      <li>
        <b>Response Streaming</b>
        - Stream responses for lower perceived latency
      </li>
      <li>
        <b>Profiling</b>
        - Measure and optimize bottlenecks
      </li>
    </ol>
    <section><title>Performance Improvements</title></section>
    <codeblock>Before Optimization:
├─ First response: 5,000ms
├─ Cached response: 5,000ms (no cache)
├─ 10 file analysis: 15,000ms (sequential)
├─ Memory usage: 800MB
└─ Startup time: 2,000ms

After Optimization:
├─ First response: 1,500ms (3.3x faster)
├─ Cached response: 50ms (100x faster)
├─ 10 file analysis: 1,700ms (8.8x faster)
├─ Memory usage: 320MB (60% reduction)
└─ Startup time: 400ms (5x faster)
</codeblock>
    <section><title>Real-World Impact</title></section>
    <p><b>Cost Savings:</b>
- 70% cache hit rate = 70% fewer API calls
- $100/month → $30/month (70% cost reduction)</p>
    <p><b>User Experience:</b>
- 3-5x faster responses
- Instant cached results
- Lower perceived latency with streaming</p>
    <p><b>Scalability:</b>
- 10x higher concurrency
- Lower resource usage
- Better reliability</p>
    <section><title>Next Steps</title></section>
    <p>In <b>Chapter 12: Monitoring, Observability, and Reliability →</b>, you&apos;ll learn how to monitor your optimized system in production with comprehensive logging, metrics, tracing, and alerting.</p>
    <p><i>Chapter 11 | Performance Optimization | Complete</i></p>
  </body>
</topic>