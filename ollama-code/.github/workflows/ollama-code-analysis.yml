name: Ollama Code Analysis

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run weekly quality analysis
    - cron: '0 6 * * 1'

jobs:
  analysis:
    name: AI-Powered Code Analysis
    runs-on: ubuntu-latest

    permissions:
      contents: read
      pull-requests: write
      security-events: write
      actions: read

    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '20'
        cache: 'yarn'

    - name: Install Dependencies
      run: yarn install --frozen-lockfile

    - name: Build Project
      run: yarn build

    - name: Run Tests
      run: yarn test
      continue-on-error: true

    - name: Ollama Code Analysis
      id: analysis
      uses: ./.github/actions/ollama-code-analysis
      with:
        # Analysis Configuration
        enable-security: 'true'
        enable-performance: 'true'
        enable-architecture: 'true'
        enable-regression: 'true'

        # Quality Gates
        min-quality-score: '80'
        max-critical-issues: '0'
        max-security-issues: '5'
        max-performance-issues: '3'
        min-test-coverage: '80'
        regression-threshold: 'medium'

        # Reporting
        report-format: 'json'
        output-path: './reports'

        # CI Configuration
        fail-on-quality-gate: 'true'
        analysis-timeout: '300'

        # Notifications
        enable-pr-comments: 'true'
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    - name: Upload Analysis Reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: ollama-code-reports
        path: reports/
        retention-days: 30

    - name: Upload SARIF Results
      uses: github/codeql-action/upload-sarif@v3
      if: always() && steps.analysis.outputs.report-url
      with:
        sarif_file: reports/ollama-code-report.sarif
        category: ollama-code-analysis
      continue-on-error: true

    - name: Quality Gate Check
      if: steps.analysis.outputs.quality-gates-passed == 'false'
      run: |
        echo "❌ Quality gates failed!"
        echo "Overall Score: ${{ steps.analysis.outputs.quality-score }}/100"
        echo "Security Issues: ${{ steps.analysis.outputs.security-issues }}"
        echo "Performance Issues: ${{ steps.analysis.outputs.performance-issues }}"
        echo "Regression Risk: ${{ steps.analysis.outputs.regression-risk }}"
        exit 1

    - name: Success Notification
      if: steps.analysis.outputs.quality-gates-passed == 'true'
      run: |
        echo "✅ All quality gates passed!"
        echo "Overall Score: ${{ steps.analysis.outputs.quality-score }}/100"
        echo "Security Issues: ${{ steps.analysis.outputs.security-issues }}"
        echo "Performance Issues: ${{ steps.analysis.outputs.performance-issues }}"
        echo "Regression Risk: ${{ steps.analysis.outputs.regression-risk }}"

  security-baseline:
    name: Security Baseline Update
    runs-on: ubuntu-latest
    needs: analysis
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
    - name: Checkout Code
      uses: actions/checkout@v4

    - name: Update Security Baseline
      run: |
        echo "Updating security baseline with latest scan results"
        # Implementation would update baseline files

  performance-tracking:
    name: Performance Trend Tracking
    runs-on: ubuntu-latest
    needs: analysis
    if: github.event_name == 'push'

    steps:
    - name: Checkout Code
      uses: actions/checkout@v4

    - name: Track Performance Trends
      run: |
        echo "Tracking performance trends"
        echo "Quality Score: ${{ needs.analysis.outputs.quality-score }}"
        # Implementation would update trend tracking

  deployment-readiness:
    name: Deployment Readiness Check
    runs-on: ubuntu-latest
    needs: analysis
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
    - name: Check Deployment Readiness
      run: |
        if [ "${{ needs.analysis.outputs.quality-gates-passed }}" = "true" ]; then
          echo "✅ Ready for deployment"
          echo "Quality Score: ${{ needs.analysis.outputs.quality-score }}/100"
        else
          echo "❌ Not ready for deployment - quality gates failed"
          exit 1
        fi

    - name: Create Deployment
      if: success()
      run: |
        echo "Creating deployment with quality score: ${{ needs.analysis.outputs.quality-score }}"
        # Implementation would trigger deployment